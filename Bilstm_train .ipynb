
import re
import datetime
import bz2
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

import warnings
warnings.filterwarnings('ignore')

import nltk
from nltk.corpus import stopwords
from wordcloud import WordCloud, STOPWORDS
from nltk.stem.wordnet import WordNetLemmatizer


from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import CountVectorizer

from sklearn.model_selection import train_test_split

from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential 
from tensorflow.keras.layers import Embedding, Dense, Dropout, Dense, Dropout, Embedding, LSTM, Bidirectional

# def decompress_bz2 (input_file_path,output_file_path) :

#     with open(input_file_path,'rb') as input_file,open(output_file_path,'wb') as output_file :
#         decompressor = bz2.BZ2Decompressor()

#         for i in iter(lambda : input_file.read(4096),b'') :
#             output_file.write(decompressor.decompress(i))

#     print(f'Decompression complete. Output file saved at: {output_file_path}')

def structured_data(file):
    text = open(file).read().splitlines()
    data = [line.split(' ', 1) for line in text]

    for i in data:
        i[0] = int(i[0].replace('__label__', ''))


    df = pd.DataFrame(data, columns=['label', 'text'])

    return df


train_df = structured_data('../Dataset/train.ft.txt')

train_df = train_df.sample(frac=0.05, random_state=42)

print('Shape of dataset: ',train_df.shape)

print('Read dataset\n',train_df.head())

print('Check uniqueness',train_df['label'].unique())

df = train_df

print('Dataset columns: ',df.columns)

df['label'] = df['label'].map({1: 0, 2: 1})

df['label'].iloc[3]

print('Check numner of uniques: ',df['label'].nunique())

print('Check any missing values\n',df.isna().sum())

print('Dataset information\n',df.info())

df.describe

print(df['label'].value_counts())

plt.figure(figsize=(12,8))
ax = sns.countplot(data=df,x='label')
plt.xticks(rotation=75)
for p in ax.patches:
    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))
plt.show()

nltk.download('punkt')
nltk.download('wordnet')    #used as wordcloud
nltk.download('omw-1.4')
nltk.download('stopwords')

df['text'] = df['text'].apply(lambda x:x.lower())
for i in range(len(df)):
    lw=[]
    for j in df['text'].iloc[i].split():
        if len(j)>=3:
            lw.append(j)
    df['text'].iloc[i]=" ".join(lw)

ps = list(";$?.:-()[]/\'_!,")
df['text'] = df['text']

for p in ps:
    df['text'] = df['text'].str.replace(p, '')

df['text'] = df['text'].str.replace("    ", " ")
df['text'] = df['text'].str.replace('"', '')
df['text'] = df['text'].apply(lambda x: x.replace('\t', ' '))
df['text'] = df['text'].str.replace("'s", "")
df['text'] = df['text'].apply(lambda x: x.replace('\n', ' '))

df['text'] = df['text'].apply(lambda x: x.lower())
print('Reviews after removing punctuation and special characters\n',df['text'])

stop = set(STOPWORDS)

def remove_stopwords(text):
    words = text.split()  # Split the sentence into words
    filtered_words = [word for word in words if word.lower() not in stop]
    return ' '.join(filtered_words)

df['text'] = df['text'].apply(remove_stopwords)

df['text'].iloc[0]

wl = WordNetLemmatizer()

nr = len(df)
lis = []
for r in range(0, nr):
    ll = []
    t = df['text'].iloc[r]
    tw = str(t).split(" ")
    for w in tw:
        ll.append(wl.lemmatize(w, pos="v"))
    lt = " ".join(ll)
    lis.append(lt)

print('Reviews after word lemmatization\n',lis)

df['text'] = lis

# sw = list(stopwords.words('english'))
# for s in sw:
#     rs = r"\b" + s + r"\b"
#     df['text'] = df['text'].str.replace(rs, '')

df['text'].iloc[0]

X = df.text.values
y = df.label.values

print('Features',X)

print('Target values',y)


tokenizer = Tokenizer(num_words=1000)
tokenizer.fit_on_texts(X)

X = tokenizer.texts_to_sequences(X)

max_len =  1000
X = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen=max_len, padding='post')

vocab_size = len(tokenizer.word_index)+1
print('Vocabulary size: ',vocab_size)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=128),
    Bidirectional(LSTM(64,return_sequences=True)),
    Dropout(0.2),
    Bidirectional(LSTM(32, return_sequences=True)),
    Dropout(0.2),
    Bidirectional(LSTM(16)),
    Dense(1, activation='sigmoid')
])

model.summary()

# Compile the model
model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

history = model.fit(X_train, y_train , validation_data=(X_test, y_test), epochs=5, batch_size=64)

# Save the model
# model.save('../SavedFiles/model.h5')

# Get predictions on test data
y_pred = (model.predict(X_test) > 0.5).astype("int32")


# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')

# F1 Score
f1 = f1_score(y_test, y_pred)
print(f'F1 Score: {f1}')

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print(f'Confusion Matrix:\n{conf_matrix}')

# Classification Report
class_report = classification_report(y_test, y_pred)
print(f'Classification Report:\n{class_report}')

plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.xlabel('Epochs')
plt.ylabel('Metrics')
plt.show()
